{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dq315\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork:\n",
    "    def __init__(self, sess, n_actions, n_features, learning_rate, gamma, replace_target_iter):\n",
    "        self.sess = sess\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.learn_step_counter = 0\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        \n",
    "        # inputs \n",
    "        self.S = tf.placeholder(tf.float32, [None, self.n_features], name='s')  \n",
    "        self.S_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    \n",
    "        self.A = tf.placeholder(tf.int32, [None, ], name='a')\n",
    "        self.R = tf.placeholder(tf.float32, [None, ], name='r')\n",
    "        self.D = tf.placeholder(tf.bool, [None, ], name='done')\n",
    "        \n",
    "        # variables\n",
    "        with tf.variable_scope('net'):\n",
    "            self.q = self._build_net(self.S, scope='eval', trainable=True)\n",
    "            self.q_ = self._build_net(self.S_, scope='target', trainable=False)\n",
    "        \n",
    "        # parameters for target_net and evaluate_net\n",
    "        self.e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='net/eval')\n",
    "        self.t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='net/target')\n",
    "        \n",
    "        # target_net hard replacement\n",
    "        self.target_replace = [tf.assign(t, e) for t, e in zip(self.t_params, self.e_params)]\n",
    "        \n",
    "        # step 1: compute q'(s',a') >> s' from batch, a' from target_net (amax)\n",
    "        q_target = self.R if self.D is True else self.R + self.gamma * tf.reduce_max(self.q_, axis=1) \n",
    "        # step 2: compute q(s,a) >> s from batch, a from batch\n",
    "        a_indices = tf.stack([tf.range(batch_size), self.A], axis=1)\n",
    "        q_pred = tf.gather_nd(params=self.q, indices=a_indices) \n",
    "        # step 3: compute td_error & train \n",
    "        self.td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q_pred)\n",
    "        self.train = tf.train.AdamOptimizer(self.lr).minimize(self.td_error, var_list=self.e_params) \n",
    "        \n",
    "    def _build_net(self, s, scope, trainable):\n",
    "        with tf.variable_scope(scope):\n",
    "            init_w = tf.contrib.layers.xavier_initializer()\n",
    "            init_b = tf.constant_initializer(0.1)\n",
    "            net = tf.layers.dense(s, 24, tf.nn.relu, \n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, \n",
    "                                  name='l1', trainable=trainable)\n",
    "            net = tf.layers.dense(net, 24, tf.nn.relu, \n",
    "                                  kernel_initializer=init_w, bias_initializer=init_b, \n",
    "                                  name='l2', trainable=trainable)\n",
    "            q = tf.layers.dense(net, self.n_actions, \n",
    "                                kernel_initializer=init_w, bias_initializer=init_b, \n",
    "                                name='q', trainable=trainable)\n",
    "            return q\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = random.randrange(self.n_actions)\n",
    "        else:\n",
    "            actions_value = self.sess.run(self.q, feed_dict={self.S: s[np.newaxis, :]})\n",
    "            action = np.argmax(actions_value)\n",
    "        return action\n",
    "\n",
    "    def learn(self, bs, ba, br, bs_, bd):\n",
    "        # train evaluate_net and get loss\n",
    "        self.sess.run(self.train, feed_dict={self.S: bs, self.A: ba, self.R: br, self.S_: bs_, self.D: bd})        \n",
    "        l = self.sess.run(self.td_error, feed_dict={self.S: bs, self.A: ba, self.R: br, self.S_: bs_, self.D: bd})\n",
    "        self.learn_step_counter += 1\n",
    "        return l\n",
    "    \n",
    "    def update_target(self):\n",
    "        # check to replace target parameters\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.target_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, capacity, dims):\n",
    "        self.capacity = capacity\n",
    "        self.memory = np.zeros((capacity, dims))\n",
    "        self.counter = 0\n",
    "\n",
    "    def store_transition(self, s, a, r, s_, done):\n",
    "        transition = np.hstack((s, [a], [r], s_, [done]))\n",
    "        index = self.counter % self.capacity  \n",
    "        self.memory[index, :] = transition\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample(self, n):\n",
    "        indices = np.random.choice(np.minimum(self.capacity, self.counter), size=n)\n",
    "        return self.memory[indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features: 4\n",
      "n_actions: 2\n",
      "batch dims: 11\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')    # In case of CartPole-v1, maximum length of episode is 500\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "# network parameters\n",
    "n_actions = env.action_space.n\n",
    "n_features = env.observation_space.shape[0]\n",
    "print('n_features:', n_features)\n",
    "print('n_actions:', n_actions)\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "replace_target_iter = 1\n",
    "\n",
    "# exploration parameters\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.999\n",
    "        \n",
    "# reply buffer parameters\n",
    "capacity = 2000\n",
    "train_start = 1000\n",
    "batch_size = 64\n",
    "dims = n_features * 2 + 1 + 1 + 1\n",
    "print('batch dims:', dims)\n",
    "\n",
    "EPISODES = 200\n",
    "\n",
    "# dqn agent & reply buffer\n",
    "sess = tf.Session()\n",
    "dqn = DeepQNetwork(sess, n_actions, n_features, learning_rate, gamma, replace_target_iter)\n",
    "M = Memory(capacity, dims)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Reward: 57.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 2 Reward: 13.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 3 Reward: 82.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 4 Reward: 9.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 5 Reward: 15.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 6 Reward: 14.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 7 Reward: 16.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 8 Reward: 17.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 9 Reward: 11.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 10 Reward: 19.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 11 Reward: 15.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 12 Reward: 14.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 13 Reward: 18.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 14 Reward: 23.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 15 Reward: 30.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 16 Reward: 9.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 17 Reward: 33.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 18 Reward: 21.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 19 Reward: 15.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 20 Reward: 50.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 21 Reward: 64.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 22 Reward: 57.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 23 Reward: 26.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 24 Reward: 10.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 25 Reward: 8.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 26 Reward: 44.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 27 Reward: 13.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 28 Reward: 38.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 29 Reward: 25.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 30 Reward: 12.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 31 Reward: 12.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 32 Reward: 11.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 33 Reward: 26.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 34 Reward: 66.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 35 Reward: 32.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 36 Reward: 17.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 37 Reward: 20.0 Loss: 0 Epsilon: 1.0000\n",
      "Episode: 38 Reward: 20.0 Loss: 421 Epsilon: 0.9792\n",
      "Episode: 39 Reward: 25.0 Loss: 317 Epsilon: 0.9541\n",
      "Episode: 40 Reward: 18.0 Loss: 426 Epsilon: 0.9361\n",
      "Episode: 41 Reward: 14.0 Loss: 353 Epsilon: 0.9222\n",
      "Episode: 42 Reward: 17.0 Loss: 475 Epsilon: 0.9057\n",
      "Episode: 43 Reward: 14.0 Loss: 405 Epsilon: 0.8922\n",
      "Episode: 44 Reward: 13.0 Loss: 456 Epsilon: 0.8798\n",
      "Episode: 45 Reward: 15.0 Loss: 456 Epsilon: 0.8658\n",
      "Episode: 46 Reward: 23.0 Loss: 357 Epsilon: 0.8453\n",
      "Episode: 47 Reward: 37.0 Loss: 370 Epsilon: 0.8137\n",
      "Episode: 48 Reward: 20.0 Loss: 382 Epsilon: 0.7968\n",
      "Episode: 49 Reward: 17.0 Loss: 342 Epsilon: 0.7826\n",
      "Episode: 50 Reward: 25.0 Loss: 427 Epsilon: 0.7625\n",
      "Episode: 51 Reward: 17.0 Loss: 356 Epsilon: 0.7489\n",
      "Episode: 52 Reward: 10.0 Loss: 465 Epsilon: 0.7407\n"
     ]
    }
   ],
   "source": [
    "episode_r = []\n",
    "cost_his = []\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    step_r = 0\n",
    "    step_loss = 0\n",
    "\n",
    "    # initial observation\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        step_count += 1\n",
    "\n",
    "        # fresh env\n",
    "        if True:\n",
    "            env.render()\n",
    "\n",
    "        # RL choose action based on state\n",
    "        action = dqn.choose_action(state)\n",
    "\n",
    "        # RL take action and get next state and reward\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # CartPole-v1 >> if an action make the episode end, then gives penalty of -100\n",
    "        reward = reward if not done or step_r == 499 else -100\n",
    "        \n",
    "        # store <s, a, r, s_>\n",
    "        M.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "        # train eval_net, epsilon decay, update target_net\n",
    "        if M.counter >= train_start:\n",
    "\n",
    "            # sample batch\n",
    "            bt = M.sample(batch_size)\n",
    "            bs = bt[:, :n_features]\n",
    "            ba = bt[:, n_features: n_features + 1]\n",
    "            br = bt[:, -n_features - 2: -n_features -1]\n",
    "            bs_ = bt[:, -n_features -1: -1]\n",
    "            bd = bt[:, -1:]           \n",
    "            ba = np.hstack(ba).astype(int) #(batch_size,)\n",
    "            br = np.hstack(br) #(batch_size,)\n",
    "            bd = np.hstack(bd) #(batch_size,)\n",
    "            # train\n",
    "            tra_results = dqn.learn(bs, ba, br, bs_, bd)\n",
    "            step_loss += tra_results\n",
    "            cost_his.append(step_loss)\n",
    "            \n",
    "            # epsilon decay per train\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "            \n",
    "            # update target network every C steps\n",
    "            #dqn.update_target()\n",
    "\n",
    "        # swap observation\n",
    "        state = next_state\n",
    "\n",
    "        # record step reward\n",
    "        step_r += reward\n",
    "\n",
    "        # break while loop when end of this episode\n",
    "        if done:\n",
    "            \n",
    "            # update target network at the end of the epsiode game has a better performance\n",
    "            dqn.update_target()\n",
    "            \n",
    "            step_r = step_r if step_r == 500 else step_r + 100\n",
    "            episode_r.append(step_r)\n",
    "\n",
    "            print('Episode:', episode+1, \n",
    "                  'Reward:', step_r,\n",
    "                  'Loss:', '%.0f'%(step_loss/step_count),\n",
    "                  'Epsilon:', '%.4f'%epsilon,)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "plt.plot(np.arange(len(episode_r)), episode_r)\n",
    "plt.ylabel('Reward',fontsize=20)\n",
    "plt.xlabel('training episodes',fontsize=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 8))\n",
    "plt.plot(np.arange(len(cost_his)), cost_his)\n",
    "plt.ylabel('Loss',fontsize=20)\n",
    "plt.xlabel('training steps',fontsize=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr: [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "\n",
      "a: [0 1 2 3]\n",
      "\n",
      "b: [1 1 2 2]\n",
      "\n",
      "[ 1  4  8 11]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.arange(12).reshape(4, 3)\n",
    "print('arr:', arr)\n",
    "print('')\n",
    "\n",
    "a = np.arange(4)\n",
    "print('a:', a)\n",
    "print('')\n",
    "\n",
    "b = np.array([1,1,2,2])\n",
    "print('b:', b)\n",
    "print('')\n",
    "\n",
    "x = arr[a,b]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dq315\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr: [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "\n",
      "a: [0 1 2 3]\n",
      "\n",
      "b: [1 1 2 2]\n",
      "\n",
      "x: [ 1  4  8 11]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "arr = np.arange(12).reshape(4, 3)\n",
    "print('arr:', arr)\n",
    "print('')\n",
    "\n",
    "a = np.arange(4)\n",
    "print('a:', (a))\n",
    "print('')\n",
    "\n",
    "b = np.array([1,1,2,2])\n",
    "print('b:', b)\n",
    "print('')\n",
    "\n",
    "x = arr[a,b]\n",
    "print('x:', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr: [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "\n",
      "a: [0 1 2 3]\n",
      "\n",
      "b: [1 1 2 2]\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e97b0c07f104>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "arr = np.arange(12).reshape(4, 3)\n",
    "print('arr:', arr)\n",
    "print('')\n",
    "\n",
    "a = tf.range(4, dtype=tf.int32)\n",
    "print('a:', (sess.run(a)))\n",
    "print('')\n",
    "\n",
    "b = np.array([1,1,2,2])\n",
    "print('b:', b)\n",
    "print('')\n",
    "\n",
    "x = arr[a,b]\n",
    "print('x:', x)\n",
    "print('x:', sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
